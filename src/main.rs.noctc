use burn::backend::{Autodiff, NdArray};
use burn::{
    config::Config,
    module::Module,
    nn,
    optim::{AdamConfig, GradientsParams, Optimizer},
    record::{DefaultFileRecorder, FullPrecisionSettings},
    tensor::{Int, Tensor, TensorData, backend::Backend},
};
use cpal::traits::{DeviceTrait, HostTrait, StreamTrait};
use hound::WavReader;
use rand::seq::SliceRandom;
use rand::thread_rng;
use rustfft::{FftPlanner, num_complex::Complex};
use std::collections::VecDeque;
use std::fs;
use std::io::Write;
use std::path::Path;
use std::sync::{Arc, Mutex};

type TrainBackend = Autodiff<NdArray>;
type InferenceBackend = NdArray;

// Audio constants
const SAMPLE_RATE: u32 = 16000;
const FRAME_SIZE: usize = 512;
const HOP_SIZE: usize = 160;
const N_MELS: usize = 80;
const N_FFT: usize = 512;

// Model constants
const VOCAB_SIZE: usize = 30;
const HIDDEN_DIM: usize = 128;

// Training constants
const TRAIN_SPLIT: f32 = 0.8;
const LEARNING_RATE: f64 = 0.00001;
const EPOCHS: usize = 300;
const BATCH_LOG_INTERVAL: usize = 100;
const VALIDATION_INTERVAL: usize = 5;
const MAX_SEQ_LENGTH: usize = 200;
const MIN_SEQ_LENGTH: usize = 3;
// const MAX_SAMPLES: Option<usize> = Some(700); // Limit for debugging
const MAX_SAMPLES: Option<usize> = None; // Limit for debugging
const EARLY_STOP_LOSS: f32 = 0.1;
const SHUFFLE_DATA: bool = true;

// Simple vocabulary
const VOCAB: [&str; 30] = [
    " ", "a", "b", "c", "d", "e", "f", "g", "h", "i", "j", "k", "l", "m", "n", "o", "p", "q", "r",
    "s", "t", "u", "v", "w", "x", "y", "z", ".", "!", "?",
];

#[derive(Config)]
pub struct WhisperConfig {
    vocab_size: usize,
    feature_dim: usize,
    hidden_dim: usize,
}

#[derive(Module, Debug)]
pub struct TinyWhisper<B: Backend> {
    encoder: nn::Linear<B>,
    lstm: nn::lstm::Lstm<B>,
    decoder: nn::Linear<B>,
}

impl<B: Backend> TinyWhisper<B> {
    pub fn new(config: &WhisperConfig, device: &B::Device) -> Self {
        let encoder = nn::LinearConfig::new(config.feature_dim, config.hidden_dim).init(device);
        let lstm =
            nn::lstm::LstmConfig::new(config.hidden_dim, config.hidden_dim, false).init(device);
        let decoder = nn::LinearConfig::new(config.hidden_dim, config.vocab_size).init(device);

        Self {
            encoder,
            lstm,
            decoder,
        }
    }

    pub fn forward(&self, features: Tensor<B, 3>) -> Tensor<B, 3> {
        let encoded = self.encoder.forward(features);
        let encoded_t = encoded.swap_dims(0, 1);
        let (output, _) = self.lstm.forward(encoded_t, None);
        let output = output.swap_dims(0, 1);
        self.decoder.forward(output)
    }
}

// Fixed resampling with proper interpolation
fn resample_audio(samples: &[f32], from_rate: u32, to_rate: u32) -> Vec<f32> {
    if from_rate == to_rate {
        return samples.to_vec();
    }

    let ratio = from_rate as f64 / to_rate as f64;
    let output_len = (samples.len() as f64 / ratio) as usize;
    let mut output = Vec::with_capacity(output_len);

    for i in 0..output_len {
        let src_idx = i as f64 * ratio;
        let idx = src_idx as usize;

        if idx + 1 < samples.len() {
            let frac = src_idx - idx as f64;
            let sample = samples[idx] * (1.0 - frac as f32) + samples[idx + 1] * frac as f32;
            output.push(sample);
        } else if idx < samples.len() {
            output.push(samples[idx]);
        } else {
            break;
        }
    }

    output
}

// Simplified audio loading - ONLY handle WAV files
fn load_audio(path: &Path) -> Result<Vec<f32>, String> {
    // Only accept WAV files
    if path.extension().and_then(|s| s.to_str()) != Some("wav") {
        return Err(format!("Only WAV files are supported, got: {:?}", path));
    }

    let mut reader =
        WavReader::open(path).map_err(|e| format!("Failed to open WAV file: {}", e))?;

    let spec = reader.spec();

    // Verify format
    if spec.channels != 1 {
        return Err(format!(
            "Expected mono audio, got {} channels",
            spec.channels
        ));
    }

    if spec.sample_rate != SAMPLE_RATE {
        return Err(format!(
            "Expected {}Hz sample rate, got {}Hz",
            SAMPLE_RATE, spec.sample_rate
        ));
    }

    if spec.sample_format != hound::SampleFormat::Int || spec.bits_per_sample != 16 {
        return Err(format!(
            "Expected 16-bit integer samples, got {:?} {}-bit",
            spec.sample_format, spec.bits_per_sample
        ));
    }

    // Read samples - we know they're i16 now
    let samples: Vec<f32> = reader
        .samples::<i16>()
        .map(|s| {
            let sample = s.map_err(|e| format!("Failed to read sample: {}", e))?;
            // Normalize to [-1, 1]
            Ok(sample as f32 / 32768.0)
        })
        .collect::<Result<Vec<_>, String>>()?;

    // Verify we got samples
    if samples.is_empty() {
        return Err("No samples in audio file".to_string());
    }

    // Check for silence
    let rms = (samples.iter().map(|&x| x * x).sum::<f32>() / samples.len() as f32).sqrt();
    if rms < 0.001 {
        eprintln!(
            "⚠️  Warning: Audio file appears to be silent (RMS: {})",
            rms
        );
    }

    Ok(samples)
}

// Simplified mel spectrogram computation
fn compute_mel_spectrogram(audio: &[f32]) -> Vec<Vec<f32>> {
    if audio.is_empty() {
        return Vec::new();
    }

    let mut planner = FftPlanner::new();
    let fft = planner.plan_fft_forward(N_FFT);
    let mut mel_frames = Vec::new();

    // Pre-compute mel filterbank
    let mel_filters = create_mel_filterbank(N_FFT / 2 + 1, N_MELS, SAMPLE_RATE);

    for i in (0..audio.len().saturating_sub(FRAME_SIZE)).step_by(HOP_SIZE) {
        let frame = &audio[i..i + FRAME_SIZE];

        // Apply Hann window
        let mut windowed: Vec<f32> = frame
            .iter()
            .enumerate()
            .map(|(j, &sample)| {
                let window = 0.5
                    - 0.5 * (2.0 * std::f32::consts::PI * j as f32 / (FRAME_SIZE - 1) as f32).cos();
                sample * window
            })
            .collect();

        // Pad to FFT size
        windowed.resize(N_FFT, 0.0);

        // Convert to complex for FFT
        let mut complex_buffer: Vec<Complex<f32>> =
            windowed.iter().map(|&s| Complex::new(s, 0.0)).collect();

        // Perform FFT
        fft.process(&mut complex_buffer);

        // Compute power spectrum (only positive frequencies)
        let power_spectrum: Vec<f32> = complex_buffer[..=N_FFT / 2]
            .iter()
            .map(|c| c.norm_sqr())
            .collect();

        // Apply mel filterbank
        let mut mel_frame = vec![0.0; N_MELS];
        for (i, mel_filter) in mel_filters.iter().enumerate() {
            mel_frame[i] = power_spectrum
                .iter()
                .zip(mel_filter.iter())
                .map(|(&p, &f)| p * f)
                .sum::<f32>()
                .max(1e-10) // Avoid log(0)
                .log10();
        }

        mel_frames.push(mel_frame);
    }

    mel_frames
}

// Create mel filterbank
fn create_mel_filterbank(n_fft: usize, n_mels: usize, sample_rate: u32) -> Vec<Vec<f32>> {
    let fmax = sample_rate as f32 / 2.0;
    let mel_max = 2595.0 * (1.0 + fmax / 700.0).log10();
    let mel_points: Vec<f32> = (0..=n_mels + 1)
        .map(|i| i as f32 * mel_max / (n_mels + 1) as f32)
        .collect();

    let hz_points: Vec<f32> = mel_points
        .iter()
        .map(|&mel| 700.0 * (10.0_f32.powf(mel / 2595.0) - 1.0))
        .collect();

    let bin_points: Vec<usize> = hz_points
        .iter()
        .map(|&hz| ((n_fft as f32) * hz / sample_rate as f32).round() as usize)
        .collect();

    let mut filterbank = vec![vec![0.0; n_fft]; n_mels];

    for i in 0..n_mels {
        let left = bin_points[i];
        let center = bin_points[i + 1];
        let right = bin_points[i + 2];

        for j in left..center {
            if j < n_fft {
                filterbank[i][j] = (j - left) as f32 / (center - left) as f32;
            }
        }

        for j in center..right {
            if j < n_fft {
                filterbank[i][j] = (right - j) as f32 / (right - center) as f32;
            }
        }
    }

    filterbank
}

fn text_to_tokens(text: &str) -> Vec<i64> {
    let tokens: Vec<i64> = text
        .chars()
        .filter_map(|c| {
            let c_lower = c.to_lowercase().to_string();
            VOCAB
                .iter()
                .position(|&v| v == c_lower)
                .map(|idx| idx as i64)
        })
        .collect();

    println!("   Text: \"{}\" -> {} tokens", text, tokens.len());
    tokens
}

fn decode_predictions(predictions: Tensor<InferenceBackend, 3>) -> String {
    let pred_data = predictions.argmax(2).into_data();
    let indices = pred_data.as_slice::<i64>().unwrap();

    let mut result = String::new();
    let mut prev_idx = -1i64;

    for &idx in indices {
        if idx != prev_idx && idx >= 0 && (idx as usize) < VOCAB_SIZE {
            if let Some(&ch) = VOCAB.get(idx as usize) {
                result.push_str(ch);
            }
        }
        prev_idx = idx;
    }
    result
}

// Updated evaluate function to return both loss and accuracy
fn evaluate_model_with_accuracy(
    model: &TinyWhisper<InferenceBackend>,
    data: &[(Vec<Vec<f32>>, Vec<i64>)],
    device: &<InferenceBackend as Backend>::Device,
) -> (f32, f32) {
    let mut total_loss = 0.0;
    let mut total_correct = 0u64;
    let mut total_chars = 0u64;
    let mut count = 0;

    for (mel_frames, tokens) in data.iter() {
        let seq_len = mel_frames.len().min(tokens.len()).min(MAX_SEQ_LENGTH);
        if seq_len < MIN_SEQ_LENGTH {
            continue;
        }

        let features_vec: Vec<f32> = mel_frames[..seq_len].iter().flatten().copied().collect();
        let features = Tensor::<InferenceBackend, 3>::from_data(
            TensorData::new(features_vec, [1, seq_len, N_MELS]),
            device,
        );

        let targets = Tensor::<InferenceBackend, 2, Int>::from_data(
            TensorData::new(tokens[..seq_len].to_vec(), [1, seq_len]),
            device,
        );

        let logits = model.forward(features);
        let [b, s, v] = logits.dims();

        // Calculate accuracy
        let predictions = logits.clone().argmax(2);
        let pred_data = predictions.into_data();
        let pred_indices = pred_data.as_slice::<i64>().unwrap();

        for (pred, &target) in pred_indices.iter().zip(tokens[..seq_len].iter()) {
            if *pred == target {
                total_correct += 1;
            }
            total_chars += 1;
        }

        let loss = nn::loss::CrossEntropyLossConfig::new()
            .init(device)
            .forward(logits.reshape([b * s, v]), targets.reshape([b * s]));

        total_loss += loss.into_scalar();
        count += 1;
    }

    let avg_loss = if count > 0 {
        total_loss / count as f32
    } else {
        f32::INFINITY
    };

    let accuracy = if total_chars > 0 {
        (total_correct as f32 / total_chars as f32) * 100.0
    } else {
        0.0
    };

    (avg_loss, accuracy)
}

// Training function with enhanced debugging
fn train_model(dataset_path: &str) {
    println!("🎯 Starting training with configuration:");
    println!("   Learning rate: {}", LEARNING_RATE);
    println!("   Epochs: {}", EPOCHS);
    println!(
        "   Train/Val split: {}/{}%",
        (TRAIN_SPLIT * 100.0) as u32,
        ((1.0 - TRAIN_SPLIT) * 100.0) as u32
    );
    println!("   Max sequence length: {}", MAX_SEQ_LENGTH);
    println!("   Max samples: {:?}", MAX_SAMPLES);

    let device = Default::default();
    let inference_device: <InferenceBackend as Backend>::Device = Default::default();

    let config = WhisperConfig {
        vocab_size: VOCAB_SIZE,
        feature_dim: N_MELS,
        hidden_dim: HIDDEN_DIM,
    };

    let mut model = TinyWhisper::<TrainBackend>::new(&config, &device);
    let mut optimizer = AdamConfig::new().init();

    println!("\n📁 Loading dataset from: {}", dataset_path);
    let mut all_data = Vec::new();
    let mut load_errors = 0;

    for (idx, entry) in fs::read_dir(dataset_path).unwrap().enumerate() {
        if let Some(max) = MAX_SAMPLES {
            if idx >= max {
                break;
            }
        }

        let entry = entry.unwrap();
        let path = entry.path();

        if path.extension().and_then(|s| s.to_str()) == Some("wav") {
            let transcript_path = path.with_extension("txt");
            if transcript_path.exists() {
                // Load audio with error handling
                match load_audio(&path) {
                    Ok(audio) => {
                        let transcript = fs::read_to_string(&transcript_path)
                            .unwrap()
                            .trim()
                            .to_lowercase();

                        if transcript.len() >= 2 {
                            let mel_frames = compute_mel_spectrogram(&audio);
                            let tokens = text_to_tokens(&transcript);

                            if mel_frames.len() > 10 && !tokens.is_empty() {
                                all_data.push((mel_frames, tokens));

                                if all_data.len() <= 5 {
                                    println!(
                                        "  ✓ Loaded: {} -> \"{}\"",
                                        path.file_name().unwrap().to_string_lossy(),
                                        transcript
                                    );
                                }
                            }
                        }
                    }
                    Err(e) => {
                        eprintln!("  ❌ Failed to load {}: {}", path.display(), e);
                        load_errors += 1;
                    }
                }
            }
        }
    }

    if load_errors > 0 {
        println!("⚠️  Warning: Failed to load {} files", load_errors);
    }

    // Shuffle before splitting
    let mut rng = thread_rng();
    all_data.shuffle(&mut rng);

    // Split into train and validation
    let split_idx = (all_data.len() as f32 * TRAIN_SPLIT) as usize;
    let (training_data, validation_data) = all_data.split_at(split_idx);
    let training_data = training_data.to_vec();
    let validation_data = validation_data.to_vec();

    println!("\n📊 Dataset loaded:");
    println!("   Total samples: {}", all_data.len());
    println!("   Training samples: {}", training_data.len());
    println!("   Validation samples: {}", validation_data.len());

    // Training loop
    let mut best_val_loss = f32::INFINITY;
    let mut best_val_acc = 0.0f32;
    let mut epochs_without_improvement = 0;

    for epoch in 0..EPOCHS {
        let mut total_loss = 0.0;
        let mut total_correct = 0u64;
        let mut total_chars = 0u64;
        let start_time = std::time::Instant::now();

        // Shuffle training data each epoch if enabled
        let mut epoch_data = training_data.clone();
        if SHUFFLE_DATA {
            epoch_data.shuffle(&mut rng);
        }

        // Training
        for (idx, (mel_frames, tokens)) in epoch_data.iter().enumerate() {
            let seq_len = mel_frames.len().min(tokens.len()).min(MAX_SEQ_LENGTH);
            if seq_len < MIN_SEQ_LENGTH {
                continue;
            }

            let features_vec: Vec<f32> = mel_frames[..seq_len].iter().flatten().copied().collect();
            let features = Tensor::<TrainBackend, 3>::from_data(
                TensorData::new(features_vec, [1, seq_len, N_MELS]),
                &device,
            );

            let targets = Tensor::<TrainBackend, 2, Int>::from_data(
                TensorData::new(tokens[..seq_len].to_vec(), [1, seq_len]),
                &device,
            );

            let logits = model.forward(features);
            let [b, s, v] = logits.dims();

            // Calculate accuracy before backward pass
            let predictions = logits.clone().argmax(2);
            let pred_data = predictions.into_data();
            let pred_indices = pred_data.as_slice::<i64>().unwrap();

            // Count correct predictions
            for (pred, &target) in pred_indices.iter().zip(tokens[..seq_len].iter()) {
                if *pred == target {
                    total_correct += 1;
                }
                total_chars += 1;
            }

            // Compute loss
            let loss = nn::loss::CrossEntropyLossConfig::new()
                .init(&device)
                .forward(logits.reshape([b * s, v]), targets.reshape([b * s]));

            let loss_val = loss.clone().mean().into_scalar();
            total_loss += loss_val;

            let grads = loss.backward();
            let grads = GradientsParams::from_grads(grads, &model);
            model = optimizer.step(LEARNING_RATE, model, grads);
        }

        let train_loss = total_loss / training_data.len() as f32;
        let train_acc = (total_correct as f32 / total_chars.max(1) as f32) * 100.0;
        let epoch_time = start_time.elapsed();

        print!("Epoch {}/{}: ", epoch + 1, EPOCHS);
        print!(
            "Train Loss = {:.4}, Train Acc = {:.1}%, ",
            train_loss, train_acc
        );

        // Validation
        if epoch % VALIDATION_INTERVAL == 0 && !validation_data.is_empty() {
            let recorder = DefaultFileRecorder::<FullPrecisionSettings>::new();
            model.clone().save_file("model_temp", &recorder).unwrap();

            let inference_model = TinyWhisper::<InferenceBackend>::new(&config, &inference_device)
                .load_file("model_temp", &recorder, &inference_device)
                .unwrap();

            let (val_loss, val_acc) =
                evaluate_model_with_accuracy(&inference_model, &validation_data, &inference_device);
            print!("Val Loss = {:.4}, Val Acc = {:.1}%, ", val_loss, val_acc);

            if val_acc > best_val_acc {
                best_val_acc = val_acc;
                best_val_loss = val_loss;
                epochs_without_improvement = 0;

                model
                    .clone()
                    .save_file("model_best", &recorder)
                    .expect("Failed to save best model");
                print!("💾 (saved best), ");
            } else {
                epochs_without_improvement += 1;
            }

            let _ = std::fs::remove_file("model_temp.mpk");
        }

        println!("Time = {:.1}s", epoch_time.as_secs_f32());

        if train_acc > 95.0 {
            println!("\n✨ Early stopping - high accuracy reached!");
            break;
        }

        if epochs_without_improvement > 30 {
            println!("\n⚠️  Early stopping - no improvement for 15 epochs");
            break;
        }

        if epoch == 0 {
            let estimated_total = epoch_time.as_secs() * EPOCHS as u64;
            println!("   Estimated total time: {} minutes", estimated_total / 60);
        }
    }

    let recorder = DefaultFileRecorder::<FullPrecisionSettings>::new();
    model
        .save_file("model", &recorder)
        .expect("Failed to save model");

    println!("\n✅ Training complete!");
    println!("   Final model saved to 'model.mpk'");
    if best_val_acc > 0.0 {
        println!(
            "   Best model saved to 'model_best.mpk' (val acc: {:.1}%, loss: {:.4})",
            best_val_acc, best_val_loss
        );
    }
}

// Real-time transcription (keeping existing implementation)
fn transcribe_realtime() {
    println!("🎤 Loading model and starting transcription...");

    let device = Default::default();
    let config = WhisperConfig {
        vocab_size: VOCAB_SIZE,
        feature_dim: N_MELS,
        hidden_dim: HIDDEN_DIM,
    };

    let recorder = DefaultFileRecorder::<FullPrecisionSettings>::new();
    let model = TinyWhisper::<InferenceBackend>::new(&config, &device)
        .load_file("model_best", &recorder, &device)
        .or_else(|_| {
            println!("⚠️  Best model not found, loading regular model...");
            TinyWhisper::<InferenceBackend>::new(&config, &device)
                .load_file("model", &recorder, &device)
        })
        .expect("Failed to load model. Train first with: cargo run -- train <dataset_path>");

    println!("✅ Model loaded successfully");

    let audio_buffer = Arc::new(Mutex::new(VecDeque::<f32>::new()));
    let audio_buffer_clone = Arc::clone(&audio_buffer);

    let host = cpal::default_host();
    let input_device = host.default_input_device().expect("No input device");
    let device_name = input_device.name().unwrap_or("Unknown".to_string());

    println!("✅ Using device: {}", device_name);

    let input_config = input_device
        .default_input_config()
        .expect("Failed to get config");
    println!(
        "📈 Config: {} channels, {}Hz",
        input_config.channels(),
        input_config.sample_rate().0
    );

    let channels = input_config.channels();

    let stream = match input_config.sample_format() {
        cpal::SampleFormat::F32 => input_device
            .build_input_stream(
                &input_config.into(),
                move |data: &[f32], _: &_| {
                    let mono_data: Vec<f32> = if channels > 1 {
                        data.chunks(channels as usize)
                            .map(|chunk| chunk.iter().sum::<f32>() / channels as f32)
                            .collect()
                    } else {
                        data.to_vec()
                    };

                    let mut buffer = audio_buffer_clone.lock().unwrap();
                    buffer.extend(mono_data.iter());
                    while buffer.len() > SAMPLE_RATE as usize * 10 {
                        buffer.pop_front();
                    }
                },
                |err| eprintln!("Stream error: {}", err),
                None,
            )
            .expect("Failed to build input stream"),
        _ => panic!("Unsupported format"),
    };

    stream.play().expect("Failed to start stream");
    println!("\n🎯 Listening... (Ctrl+C to stop)\n");

    let mut last_processed = 0;

    loop {
        std::thread::sleep(std::time::Duration::from_millis(500));

        let buffer = audio_buffer.lock().unwrap();
        let current_len = buffer.len();

        if current_len > last_processed + SAMPLE_RATE as usize / 2 {
            let chunk: Vec<f32> = buffer.iter().copied().collect();
            drop(buffer);

            let chunk_slice = &chunk[last_processed..];
            let chunk_rms =
                (chunk_slice.iter().map(|&x| x * x).sum::<f32>() / chunk_slice.len() as f32).sqrt();

            if chunk_rms > 0.001 {
                let mel_frames = compute_mel_spectrogram(chunk_slice);

                if mel_frames.len() > 10 {
                    let features_vec: Vec<f32> = mel_frames.iter().flatten().copied().collect();
                    let seq_len = mel_frames.len();
                    let features = Tensor::<InferenceBackend, 3>::from_data(
                        TensorData::new(features_vec, [1, seq_len, N_MELS]),
                        &device,
                    );

                    let predictions = model.forward(features);
                    let text = decode_predictions(predictions);

                    if !text.trim().is_empty() {
                        println!("📝 {}", text);
                    }
                }
            }

            last_processed = current_len - SAMPLE_RATE as usize / 4;
        }
    }
}

fn test_model_inference() {
    println!("🧪 Testing model inference...");

    let device = Default::default();
    let config = WhisperConfig {
        vocab_size: VOCAB_SIZE,
        feature_dim: N_MELS,
        hidden_dim: HIDDEN_DIM,
    };

    let recorder = DefaultFileRecorder::<FullPrecisionSettings>::new();
    let model = TinyWhisper::<InferenceBackend>::new(&config, &device)
        .load_file("model", &recorder, &device)
        .expect("Failed to load model");

    println!("\nTest 1: 10 frames of silence");
    let test_frames = vec![vec![-10.0; N_MELS]; 10];
    let features_vec: Vec<f32> = test_frames.iter().flatten().copied().collect();
    let features = Tensor::<InferenceBackend, 3>::from_data(
        TensorData::new(features_vec, [1, 10, N_MELS]),
        &device,
    );
    let predictions = model.forward(features);
    let decoded = decode_predictions(predictions);
    println!("Decoded: '{}'", decoded);

    println!("\nTest 2: 10 frames of varying energy");
    let test_frames2 = vec![vec![-5.0; N_MELS]; 10];
    let features_vec2: Vec<f32> = test_frames2.iter().flatten().copied().collect();
    let features2 = Tensor::<InferenceBackend, 3>::from_data(
        TensorData::new(features_vec2, [1, 10, N_MELS]),
        &device,
    );
    let predictions2 = model.forward(features2);
    let decoded2 = decode_predictions(predictions2);
    println!("Decoded: '{}'", decoded2);

    if decoded == decoded2 && decoded.chars().all(|c| c == ' ') {
        println!("\n⚠️  WARNING: Model outputs only spaces for different inputs!");
        println!("The model may need more training or the learning rate may be too high.");
    } else {
        println!("\n✅ Model produces different outputs for different inputs.");
    }
}

fn main() {
    let args: Vec<String> = std::env::args().collect();

    match args.get(1).map(|s| s.as_str()) {
        Some("train") => {
            if let Some(dataset_path) = args.get(2) {
                train_model(dataset_path);
            } else {
                eprintln!("Usage: cargo run -- train <dataset_path>");
                eprintln!("Dataset should contain .wav/.flac files with corresponding .txt files");
            }
        }
        Some("transcribe") => transcribe_realtime(),
        Some("test-model") => test_model_inference(),
        _ => {
            println!("Whisper-like Speech Recognition");
            println!("Usage:");
            println!("  cargo run -- train <dataset_path>  # Train on dataset");
            println!("  cargo run -- transcribe            # Real-time transcription");
            println!("  cargo run -- test-model            # Test model inference");
        }
    }
}

// use burn::backend::{Autodiff, NdArray};
// use burn::{
//     config::Config,
//     module::Module,
//     nn,
//     optim::{AdamConfig, GradientsParams, Optimizer},
//     record::{DefaultFileRecorder, FullPrecisionSettings},
//     tensor::{Int, Tensor, TensorData, backend::Backend},
// };
// use cpal::traits::{DeviceTrait, HostTrait, StreamTrait};
// use hound::WavReader;
// use rand::seq::SliceRandom;
// use rand::thread_rng;
// use rustfft::{FftPlanner, num_complex::Complex};
// use std::collections::VecDeque;
// use std::fs;
// use std::io::Write;
// use std::path::Path;
// use std::sync::{Arc, Mutex};

// type TrainBackend = Autodiff<NdArray>;
// type InferenceBackend = NdArray;

// // Audio constants
// const SAMPLE_RATE: u32 = 16000;
// const FRAME_SIZE: usize = 512;
// const HOP_SIZE: usize = 160;
// const N_MELS: usize = 80;
// const N_FFT: usize = 512;

// // Model constants
// const VOCAB_SIZE: usize = 30;
// const HIDDEN_DIM: usize = 128;

// // Training constants
// const TRAIN_SPLIT: f32 = 0.8; // 80% for training, 20% for validation
// const LEARNING_RATE: f64 = 0.0001;
// const EPOCHS: usize = 50;
// const BATCH_LOG_INTERVAL: usize = 100; // Log every N batches
// const VALIDATION_INTERVAL: usize = 5; // Validate every N epochs
// const MAX_SEQ_LENGTH: usize = 200; // Maximum sequence length for memory efficiency
// const MIN_SEQ_LENGTH: usize = 3; // Minimum sequence length to train on
// // const MAX_SAMPLES: Option<usize> = Some(1000); // Set to Some(1000) to limit samples for testing
// const MAX_SAMPLES: Option<usize> = None; // Set to Some(1000) to limit samples for testing
// const EARLY_STOP_LOSS: f32 = 0.1; // Stop training if loss goes below this
// const SHUFFLE_DATA: bool = true; // Shuffle training data each epoch

// // Simple vocabulary
// const VOCAB: [&str; 30] = [
//     " ", "a", "b", "c", "d", "e", "f", "g", "h", "i", "j", "k", "l", "m", "n", "o", "p", "q", "r",
//     "s", "t", "u", "v", "w", "x", "y", "z", ".", "!", "?",
// ];

// #[derive(Config)]
// pub struct WhisperConfig {
//     vocab_size: usize,
//     feature_dim: usize,
//     hidden_dim: usize,
// }

// #[derive(Module, Debug)]
// pub struct TinyWhisper<B: Backend> {
//     encoder: nn::Linear<B>,
//     lstm: nn::lstm::Lstm<B>,
//     decoder: nn::Linear<B>,
// }

// impl<B: Backend> TinyWhisper<B> {
//     pub fn new(config: &WhisperConfig, device: &B::Device) -> Self {
//         let encoder = nn::LinearConfig::new(config.feature_dim, config.hidden_dim).init(device);
//         let lstm =
//             nn::lstm::LstmConfig::new(config.hidden_dim, config.hidden_dim, false).init(device);
//         let decoder = nn::LinearConfig::new(config.hidden_dim, config.vocab_size).init(device);

//         Self {
//             encoder,
//             lstm,
//             decoder,
//         }
//     }

//     pub fn forward(&self, features: Tensor<B, 3>) -> Tensor<B, 3> {
//         let encoded = self.encoder.forward(features);
//         let encoded_t = encoded.swap_dims(0, 1);
//         let (output, _) = self.lstm.forward(encoded_t, None);
//         let output = output.swap_dims(0, 1);
//         self.decoder.forward(output)
//     }
// }

// // Better resampling function with linear interpolation
// fn resample_audio(samples: &[f32], from_rate: u32, to_rate: u32) -> Vec<f32> {
//     if from_rate == to_rate {
//         return samples.to_vec();
//     }

//     let ratio = from_rate as f64 / to_rate as f64;
//     let output_len = (samples.len() as f64 / ratio) as usize;
//     let mut output = Vec::with_capacity(output_len);

//     for i in 0..output_len {
//         let src_idx = i as f64 * ratio;
//         let idx = src_idx as usize;

//         if idx + 1 < samples.len() {
//             // Linear interpolation
//             let frac = src_idx - idx as f64;
//             let sample = samples[idx] * (1.0 - frac as f32) + samples[idx + 1] * frac as f32;
//             output.push(sample);
//         } else if idx < samples.len() {
//             output.push(samples[idx]);
//         }
//     }

//     output
// }

// fn load_audio(path: &Path) -> Vec<f32> {
//     match path.extension().and_then(|s| s.to_str()) {
//         Some("wav") => {
//             let reader = WavReader::open(path).unwrap();
//             let spec = reader.spec();
//             let samples: Vec<f32> = reader
//                 .into_samples::<i16>()
//                 .map(|s| s.unwrap() as f32 / i16::MAX as f32)
//                 .collect();

//             resample_audio(&samples, spec.sample_rate, SAMPLE_RATE)
//         }
//         Some("flac") => {
//             let mut reader = claxon::FlacReader::open(path).unwrap();
//             let info = reader.streaminfo();
//             let sample_rate = info.sample_rate;
//             let samples: Vec<f32> = reader
//                 .samples()
//                 .map(|s| s.unwrap() as f32 / i32::MAX as f32)
//                 .collect();

//             resample_audio(&samples, sample_rate, SAMPLE_RATE)
//         }
//         _ => panic!("Unsupported audio format: {:?}", path),
//     }
// }

// fn compute_mel_spectrogram(audio: &[f32]) -> Vec<Vec<f32>> {
//     let mut planner = FftPlanner::new();
//     let fft = planner.plan_fft_forward(N_FFT);
//     let mut mel_frames = Vec::new();

//     for i in (0..audio.len().saturating_sub(FRAME_SIZE)).step_by(HOP_SIZE) {
//         let frame = &audio[i..i + FRAME_SIZE];

//         let mut complex_buffer: Vec<Complex<f32>> = frame
//             .iter()
//             .enumerate()
//             .map(|(j, &sample)| {
//                 let window = 0.5
//                     - 0.5 * (2.0 * std::f32::consts::PI * j as f32 / (FRAME_SIZE - 1) as f32).cos();
//                 Complex::new(sample * window, 0.0)
//             })
//             .collect();

//         complex_buffer.resize(N_FFT, Complex::new(0.0, 0.0));
//         fft.process(&mut complex_buffer);

//         let power_spectrum: Vec<f32> = complex_buffer[..N_FFT / 2]
//             .iter()
//             .map(|c| c.norm_sqr())
//             .collect();

//         let mut mel_frame = vec![0.0; N_MELS];
//         for (i, &power) in power_spectrum.iter().enumerate() {
//             let mel_idx = (i * N_MELS / power_spectrum.len()).min(N_MELS - 1);
//             mel_frame[mel_idx] += power;
//         }

//         for val in mel_frame.iter_mut() {
//             *val = (*val + 1e-10).log10();
//         }

//         mel_frames.push(mel_frame);
//     }

//     mel_frames
// }

// fn text_to_tokens(text: &str) -> Vec<i64> {
//     text.chars()
//         .filter_map(|c| {
//             let c_lower = c.to_lowercase().to_string();
//             VOCAB
//                 .iter()
//                 .position(|&v| v == c_lower)
//                 .map(|idx| idx as i64)
//         })
//         .collect()
// }

// fn decode_predictions(predictions: Tensor<InferenceBackend, 3>) -> String {
//     let pred_data = predictions.argmax(2).into_data();
//     let indices = pred_data.as_slice::<i64>().unwrap();

//     let mut result = String::new();
//     let mut prev_idx = -1i64;

//     for &idx in indices {
//         // Simple deduplication - don't repeat same character
//         if idx != prev_idx && idx >= 0 && (idx as usize) < VOCAB_SIZE {
//             if let Some(&ch) = VOCAB.get(idx as usize) {
//                 result.push_str(ch);
//             }
//         }
//         prev_idx = idx;
//     }
//     result
// }

// // Fix the evaluate_model function - use InferenceBackend instead of generic B
// fn evaluate_model(
//     model: &TinyWhisper<InferenceBackend>,
//     data: &[(Vec<Vec<f32>>, Vec<i64>)],
//     device: &<InferenceBackend as Backend>::Device,
// ) -> f32 {
//     let mut total_loss = 0.0;
//     let mut count = 0;

//     for (mel_frames, tokens) in data.iter() {
//         let seq_len = mel_frames.len().min(tokens.len()).min(MAX_SEQ_LENGTH);
//         if seq_len < MIN_SEQ_LENGTH {
//             continue;
//         }

//         let features_vec: Vec<f32> = mel_frames[..seq_len].iter().flatten().copied().collect();
//         let features = Tensor::<InferenceBackend, 3>::from_data(
//             TensorData::new(features_vec, [1, seq_len, N_MELS]),
//             device,
//         );

//         let targets = Tensor::<InferenceBackend, 2, Int>::from_data(
//             TensorData::new(tokens[..seq_len].to_vec(), [1, seq_len]),
//             device,
//         );

//         let logits = model.forward(features);
//         let [b, s, v] = logits.dims();

//         let loss = nn::loss::CrossEntropyLossConfig::new()
//             .init(device)
//             .forward(logits.reshape([b * s, v]), targets.reshape([b * s]));

//         total_loss += loss.into_scalar();
//         count += 1;
//     }

//     if count > 0 {
//         total_loss / count as f32
//     } else {
//         f32::INFINITY
//     }
// }

// // Training function with train/validation split
// // Training function with train/validation split and accuracy tracking
// fn train_model(dataset_path: &str) {
//     println!("🎯 Starting training with configuration:");
//     println!("   Learning rate: {}", LEARNING_RATE);
//     println!("   Epochs: {}", EPOCHS);
//     println!(
//         "   Train/Val split: {}/{}%",
//         (TRAIN_SPLIT * 100.0) as u32,
//         ((1.0 - TRAIN_SPLIT) * 100.0) as u32
//     );
//     println!("   Max sequence length: {}", MAX_SEQ_LENGTH);

//     let device = Default::default();
//     let inference_device: <InferenceBackend as Backend>::Device = Default::default();

//     let config = WhisperConfig {
//         vocab_size: VOCAB_SIZE,
//         feature_dim: N_MELS,
//         hidden_dim: HIDDEN_DIM,
//     };

//     let mut model = TinyWhisper::<TrainBackend>::new(&config, &device);
//     let mut optimizer = AdamConfig::new().init();

//     // Load dataset
//     println!("\n📁 Loading dataset from: {}", dataset_path);
//     let mut all_data = Vec::new();

//     for (idx, entry) in fs::read_dir(dataset_path).unwrap().enumerate() {
//         if let Some(max) = MAX_SAMPLES {
//             if idx >= max {
//                 break;
//             }
//         }

//         let entry = entry.unwrap();
//         let path = entry.path();

//         if let Some(ext) = path.extension().and_then(|s| s.to_str()) {
//             if ext == "wav" || ext == "flac" {
//                 let transcript_path = path.with_extension("txt");
//                 if transcript_path.exists() {
//                     let audio = load_audio(&path);
//                     let transcript = fs::read_to_string(&transcript_path)
//                         .unwrap()
//                         .trim()
//                         .to_lowercase();

//                     if transcript.len() < 2 {
//                         continue;
//                     }

//                     let mel_frames = compute_mel_spectrogram(&audio);
//                     let tokens = text_to_tokens(&transcript);

//                     if mel_frames.len() > 10 && !tokens.is_empty() {
//                         all_data.push((mel_frames, tokens));
//                         println!(
//                             "  ✓ Sample {}: {} -> \"{}\"",
//                             all_data.len(),
//                             path.file_name().unwrap().to_string_lossy(),
//                             transcript
//                         );
//                     }
//                 }
//             }
//         }
//     }

//     if all_data.is_empty() {
//         panic!("❌ No training data found! Check your dataset path.");
//     }

//     // Shuffle before splitting
//     let mut rng = thread_rng();
//     all_data.shuffle(&mut rng);

//     // Split into train and validation
//     let split_idx = (all_data.len() as f32 * TRAIN_SPLIT) as usize;
//     let (training_data, validation_data) = all_data.split_at(split_idx);
//     let training_data = training_data.to_vec();
//     let validation_data = validation_data.to_vec();

//     println!("\n📊 Dataset loaded:");
//     println!("   Total samples: {}", all_data.len());
//     println!("   Training samples: {}", training_data.len());
//     println!("   Validation samples: {}", validation_data.len());

//     // Training loop
//     let mut best_val_loss = f32::INFINITY;
//     let mut best_val_acc = 0.0f32;
//     let mut epochs_without_improvement = 0;

//     for epoch in 0..EPOCHS {
//         let mut total_loss = 0.0;
//         let mut total_correct = 0u64;
//         let mut total_chars = 0u64;
//         let start_time = std::time::Instant::now();

//         // Shuffle training data each epoch if enabled
//         let mut epoch_data = training_data.clone();
//         if SHUFFLE_DATA {
//             epoch_data.shuffle(&mut rng);
//         }

//         // Training
//         for (idx, (mel_frames, tokens)) in epoch_data.iter().enumerate() {
//             if idx % BATCH_LOG_INTERVAL == 0 && idx > 0 {
//                 let current_acc = (total_correct as f32 / total_chars.max(1) as f32) * 100.0;
//                 print!(
//                     "\rEpoch {}/{}: [{}/{}] Loss: {:.4}, Acc: {:.1}%",
//                     epoch + 1,
//                     EPOCHS,
//                     idx,
//                     epoch_data.len(),
//                     total_loss / idx as f32,
//                     current_acc
//                 );
//                 std::io::stdout().flush().unwrap();
//             }

//             let seq_len = mel_frames.len().min(tokens.len()).min(MAX_SEQ_LENGTH);
//             if seq_len < MIN_SEQ_LENGTH {
//                 continue;
//             }

//             let features_vec: Vec<f32> = mel_frames[..seq_len].iter().flatten().copied().collect();
//             let features = Tensor::<TrainBackend, 3>::from_data(
//                 TensorData::new(features_vec, [1, seq_len, N_MELS]),
//                 &device,
//             );

//             let targets = Tensor::<TrainBackend, 2, Int>::from_data(
//                 TensorData::new(tokens[..seq_len].to_vec(), [1, seq_len]),
//                 &device,
//             );

//             let logits = model.forward(features);
//             let [b, s, v] = logits.dims();

//             // Calculate accuracy before backward pass
//             let predictions = logits.clone().argmax(2);
//             let pred_data = predictions.into_data();
//             let pred_indices = pred_data.as_slice::<i64>().unwrap();

//             // Count correct predictions
//             for (pred, &target) in pred_indices.iter().zip(tokens[..seq_len].iter()) {
//                 if *pred == target {
//                     total_correct += 1;
//                 }
//                 total_chars += 1;
//             }

//             // Compute loss
//             let loss = nn::loss::CrossEntropyLossConfig::new()
//                 .init(&device)
//                 .forward(logits.reshape([b * s, v]), targets.reshape([b * s]));

//             let loss_val = loss.clone().mean().into_scalar();
//             total_loss += loss_val;

//             let grads = loss.backward();
//             let grads = GradientsParams::from_grads(grads, &model);
//             model = optimizer.step(LEARNING_RATE, model, grads);
//         }

//         let train_loss = total_loss / training_data.len() as f32;
//         let train_acc = (total_correct as f32 / total_chars.max(1) as f32) * 100.0;
//         let epoch_time = start_time.elapsed();

//         print!("\rEpoch {}/{}: ", epoch + 1, EPOCHS);
//         print!(
//             "Train Loss = {:.4}, Train Acc = {:.1}%, ",
//             train_loss, train_acc
//         );

//         // Validation
//         if epoch % VALIDATION_INTERVAL == 0 && !validation_data.is_empty() {
//             // Save current model temporarily
//             let recorder = DefaultFileRecorder::<FullPrecisionSettings>::new();
//             model.clone().save_file("model_temp", &recorder).unwrap();

//             // Load as inference model
//             let inference_model = TinyWhisper::<InferenceBackend>::new(&config, &inference_device)
//                 .load_file("model_temp", &recorder, &inference_device)
//                 .unwrap();

//             let (val_loss, val_acc) =
//                 evaluate_model_with_accuracy(&inference_model, &validation_data, &inference_device);
//             print!("Val Loss = {:.4}, Val Acc = {:.1}%, ", val_loss, val_acc);

//             // Check for improvement (using accuracy as primary metric)
//             if val_acc > best_val_acc {
//                 best_val_acc = val_acc;
//                 best_val_loss = val_loss;
//                 epochs_without_improvement = 0;

//                 // Save best model
//                 model
//                     .clone()
//                     .save_file("model_best", &recorder)
//                     .expect("Failed to save best model");
//                 print!("💾 (saved best), ");
//             } else {
//                 epochs_without_improvement += 1;
//             }

//             // Clean up temp file
//             let _ = std::fs::remove_file("model_temp.mpk");
//         }

//         println!("Time = {:.1}s", epoch_time.as_secs_f32());

//         // Early stopping based on accuracy
//         if train_acc > 95.0 {
//             println!("\n✨ Early stopping - high accuracy reached!");
//             break;
//         }

//         if epochs_without_improvement > 15 {
//             println!("\n⚠️  Early stopping - no improvement for 15 epochs");
//             break;
//         }

//         // Time estimate on first epoch
//         if epoch == 0 {
//             let estimated_total = epoch_time.as_secs() * EPOCHS as u64;
//             println!("   Estimated total time: {} minutes", estimated_total / 60);
//         }
//     }

//     // Save final model
//     let recorder = DefaultFileRecorder::<FullPrecisionSettings>::new();
//     model
//         .save_file("model", &recorder)
//         .expect("Failed to save model");

//     println!("\n✅ Training complete!");
//     println!("   Final model saved to 'model.mpk'");
//     if best_val_acc > 0.0 {
//         println!(
//             "   Best model saved to 'model_best.mpk' (val acc: {:.1}%, loss: {:.4})",
//             best_val_acc, best_val_loss
//         );
//     }
// }

// // Updated evaluate function to return both loss and accuracy
// fn evaluate_model_with_accuracy(
//     model: &TinyWhisper<InferenceBackend>,
//     data: &[(Vec<Vec<f32>>, Vec<i64>)],
//     device: &<InferenceBackend as Backend>::Device,
// ) -> (f32, f32) {
//     let mut total_loss = 0.0;
//     let mut total_correct = 0u64;
//     let mut total_chars = 0u64;
//     let mut count = 0;

//     for (mel_frames, tokens) in data.iter() {
//         let seq_len = mel_frames.len().min(tokens.len()).min(MAX_SEQ_LENGTH);
//         if seq_len < MIN_SEQ_LENGTH {
//             continue;
//         }

//         let features_vec: Vec<f32> = mel_frames[..seq_len].iter().flatten().copied().collect();
//         let features = Tensor::<InferenceBackend, 3>::from_data(
//             TensorData::new(features_vec, [1, seq_len, N_MELS]),
//             device,
//         );

//         let targets = Tensor::<InferenceBackend, 2, Int>::from_data(
//             TensorData::new(tokens[..seq_len].to_vec(), [1, seq_len]),
//             device,
//         );

//         let logits = model.forward(features);
//         let [b, s, v] = logits.dims();

//         // Calculate accuracy
//         let predictions = logits.clone().argmax(2);
//         let pred_data = predictions.into_data();
//         let pred_indices = pred_data.as_slice::<i64>().unwrap();

//         for (pred, &target) in pred_indices.iter().zip(tokens[..seq_len].iter()) {
//             if *pred == target {
//                 total_correct += 1;
//             }
//             total_chars += 1;
//         }

//         let loss = nn::loss::CrossEntropyLossConfig::new()
//             .init(device)
//             .forward(logits.reshape([b * s, v]), targets.reshape([b * s]));

//         total_loss += loss.into_scalar();
//         count += 1;
//     }

//     let avg_loss = if count > 0 {
//         total_loss / count as f32
//     } else {
//         f32::INFINITY
//     };

//     let accuracy = if total_chars > 0 {
//         (total_correct as f32 / total_chars as f32) * 100.0
//     } else {
//         0.0
//     };

//     (avg_loss, accuracy)
// }

// // Real-time transcription
// fn transcribe_realtime() {
//     println!("🎤 Loading model and starting transcription...");

//     let device = Default::default();
//     let config = WhisperConfig {
//         vocab_size: VOCAB_SIZE,
//         feature_dim: N_MELS,
//         hidden_dim: HIDDEN_DIM,
//     };

//     // Try to load best model first, fallback to regular model
//     let recorder = DefaultFileRecorder::<FullPrecisionSettings>::new();
//     let model = TinyWhisper::<InferenceBackend>::new(&config, &device)
//         .load_file("model_best", &recorder, &device)
//         .or_else(|_| {
//             println!("⚠️  Best model not found, loading regular model...");
//             TinyWhisper::<InferenceBackend>::new(&config, &device)
//                 .load_file("model", &recorder, &device)
//         })
//         .expect("Failed to load model. Train first with: cargo run -- train <dataset_path>");

//     println!("✅ Model loaded successfully");

//     let audio_buffer = Arc::new(Mutex::new(VecDeque::<f32>::new()));
//     let audio_buffer_clone = Arc::clone(&audio_buffer);

//     let host = cpal::default_host();
//     let input_device = host.default_input_device().expect("No input device");
//     let device_name = input_device.name().unwrap_or("Unknown".to_string());

//     println!("✅ Using device: {}", device_name);

//     let input_config = input_device
//         .default_input_config()
//         .expect("Failed to get config");
//     println!(
//         "📈 Config: {} channels, {}Hz",
//         input_config.channels(),
//         input_config.sample_rate().0
//     );

//     let channels = input_config.channels();

//     let stream = match input_config.sample_format() {
//         cpal::SampleFormat::F32 => input_device
//             .build_input_stream(
//                 &input_config.into(),
//                 move |data: &[f32], _: &_| {
//                     let mono_data: Vec<f32> = if channels > 1 {
//                         data.chunks(channels as usize)
//                             .map(|chunk| chunk.iter().sum::<f32>() / channels as f32)
//                             .collect()
//                     } else {
//                         data.to_vec()
//                     };

//                     let mut buffer = audio_buffer_clone.lock().unwrap();
//                     buffer.extend(mono_data.iter());
//                     while buffer.len() > SAMPLE_RATE as usize * 10 {
//                         buffer.pop_front();
//                     }
//                 },
//                 |err| eprintln!("Stream error: {}", err),
//                 None,
//             )
//             .expect("Failed to build input stream"),
//         _ => panic!("Unsupported format"),
//     };

//     stream.play().expect("Failed to start stream");
//     println!("\n🎯 Listening... (Ctrl+C to stop)\n");

//     let mut last_processed = 0;

//     loop {
//         std::thread::sleep(std::time::Duration::from_millis(500));

//         let buffer = audio_buffer.lock().unwrap();
//         let current_len = buffer.len();

//         if current_len > last_processed + SAMPLE_RATE as usize / 2 {
//             let chunk: Vec<f32> = buffer.iter().copied().collect();
//             drop(buffer);

//             let chunk_slice = &chunk[last_processed..];
//             let chunk_rms =
//                 (chunk_slice.iter().map(|&x| x * x).sum::<f32>() / chunk_slice.len() as f32).sqrt();

//             // Only process if there's actual audio
//             if chunk_rms > 0.001 {
//                 let mel_frames = compute_mel_spectrogram(chunk_slice);

//                 if mel_frames.len() > 10 {
//                     let features_vec: Vec<f32> = mel_frames.iter().flatten().copied().collect();
//                     let seq_len = mel_frames.len();
//                     let features = Tensor::<InferenceBackend, 3>::from_data(
//                         TensorData::new(features_vec, [1, seq_len, N_MELS]),
//                         &device,
//                     );

//                     let predictions = model.forward(features);
//                     let text = decode_predictions(predictions);

//                     if !text.trim().is_empty() {
//                         println!("📝 {}", text);
//                     }
//                 }
//             }

//             last_processed = current_len - SAMPLE_RATE as usize / 4;
//         }
//     }
// }

// // Test model inference
// fn test_model_inference() {
//     println!("🧪 Testing model inference...");

//     let device = Default::default();
//     let config = WhisperConfig {
//         vocab_size: VOCAB_SIZE,
//         feature_dim: N_MELS,
//         hidden_dim: HIDDEN_DIM,
//     };

//     let recorder = DefaultFileRecorder::<FullPrecisionSettings>::new();
//     let model = TinyWhisper::<InferenceBackend>::new(&config, &device)
//         .load_file("model", &recorder, &device)
//         .expect("Failed to load model");

//     // Test 1: Silence
//     println!("\nTest 1: 10 frames of silence");
//     let test_frames = vec![vec![-10.0; N_MELS]; 10];
//     let features_vec: Vec<f32> = test_frames.iter().flatten().copied().collect();
//     let features = Tensor::<InferenceBackend, 3>::from_data(
//         TensorData::new(features_vec, [1, 10, N_MELS]),
//         &device,
//     );
//     let predictions = model.forward(features);
//     let decoded = decode_predictions(predictions);
//     println!("Decoded: '{}'", decoded);

//     // Test 2: Different energy levels
//     println!("\nTest 2: 10 frames of varying energy");
//     let test_frames2 = vec![vec![-5.0; N_MELS]; 10];
//     let features_vec2: Vec<f32> = test_frames2.iter().flatten().copied().collect();
//     let features2 = Tensor::<InferenceBackend, 3>::from_data(
//         TensorData::new(features_vec2, [1, 10, N_MELS]),
//         &device,
//     );
//     let predictions2 = model.forward(features2);
//     let decoded2 = decode_predictions(predictions2);
//     println!("Decoded: '{}'", decoded2);

//     if decoded == decoded2 && decoded.chars().all(|c| c == ' ') {
//         println!("\n⚠️  WARNING: Model outputs only spaces for different inputs!");
//         println!("The model may need more training or the learning rate may be too high.");
//     } else {
//         println!("\n✅ Model produces different outputs for different inputs.");
//     }
// }

// fn main() {
//     let args: Vec<String> = std::env::args().collect();

//     match args.get(1).map(|s| s.as_str()) {
//         Some("train") => {
//             if let Some(dataset_path) = args.get(2) {
//                 train_model(dataset_path);
//             } else {
//                 eprintln!("Usage: cargo run -- train <dataset_path>");
//                 eprintln!("Dataset should contain .wav/.flac files with corresponding .txt files");
//             }
//         }
//         Some("transcribe") => transcribe_realtime(),
//         Some("test-model") => test_model_inference(),
//         _ => {
//             println!("Whisper-like Speech Recognition");
//             println!("Usage:");
//             println!("  cargo run -- train <dataset_path>  # Train on dataset");
//             println!("  cargo run -- transcribe            # Real-time transcription");
//             println!("  cargo run -- test-model            # Test model inference");
//         }
//     }
// }
