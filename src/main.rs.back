use burn::backend::{Autodiff, NdArray};
use burn::{
    config::Config,
    module::Module,
    nn,
    optim::{AdamConfig, GradientsParams, Optimizer},
    record::{DefaultFileRecorder, FullPrecisionSettings}, // Added FullPrecisionSettings
    tensor::{backend::Backend, Int, Tensor, TensorData},
};
use cpal::traits::{DeviceTrait, HostTrait, StreamTrait};
use hound::WavReader;
use rustfft::{num_complex::Complex, FftPlanner};
use std::collections::VecDeque;
use std::io::Write;
use std::path::Path;
use std::sync::{Arc, Mutex};
use std::{fs, io};

type TrainBackend = Autodiff<NdArray>;
type InferenceBackend = NdArray;

// Audio constants
const SAMPLE_RATE: u32 = 16000;
const FRAME_SIZE: usize = 512;
const HOP_SIZE: usize = 160;
const N_MELS: usize = 80;
const N_FFT: usize = 512;

// Model constants
const VOCAB_SIZE: usize = 30;
const HIDDEN_DIM: usize = 128;

// Simple vocabulary
const VOCAB: [&str; 30] = [
    " ", "a", "b", "c", "d", "e", "f", "g", "h", "i", "j", "k", "l", "m", "n", "o", "p", "q", "r",
    "s", "t", "u", "v", "w", "x", "y", "z", ".", "!", "?",
];

#[derive(Config)]
pub struct WhisperConfig {
    vocab_size: usize,
    feature_dim: usize,
    hidden_dim: usize,
}

#[derive(Module, Debug)]
pub struct TinyWhisper<B: Backend> {
    encoder: nn::Linear<B>,
    lstm: nn::lstm::Lstm<B>,
    decoder: nn::Linear<B>,
}

impl<B: Backend> TinyWhisper<B> {
    pub fn new(config: &WhisperConfig, device: &B::Device) -> Self {
        let encoder = nn::LinearConfig::new(config.feature_dim, config.hidden_dim).init(device);
        let lstm =
            nn::lstm::LstmConfig::new(config.hidden_dim, config.hidden_dim, false).init(device);
        let decoder = nn::LinearConfig::new(config.hidden_dim, config.vocab_size).init(device);

        Self {
            encoder,
            lstm,
            decoder,
        }
    }

    pub fn forward(&self, features: Tensor<B, 3>) -> Tensor<B, 3> {
        let encoded = self.encoder.forward(features);
        let encoded_t = encoded.swap_dims(0, 1);
        let (output, _) = self.lstm.forward(encoded_t, None);
        let output = output.swap_dims(0, 1);
        self.decoder.forward(output)
    }
}

fn load_audio(path: &Path) -> Vec<f32> {
    match path.extension().and_then(|s| s.to_str()) {
        Some("wav") => {
            let reader = WavReader::open(path).unwrap();
            let spec = reader.spec();
            let samples: Vec<f32> = reader
                .into_samples::<i16>()
                .map(|s| s.unwrap() as f32 / i16::MAX as f32)
                .collect();

            // Resample to 16kHz if needed
            if spec.sample_rate != SAMPLE_RATE {
                let ratio = spec.sample_rate as f32 / SAMPLE_RATE as f32;
                samples
                    .iter()
                    .enumerate()
                    .filter_map(|(i, &s)| {
                        if (i as f32 % ratio) < 1.0 {
                            Some(s)
                        } else {
                            None
                        }
                    })
                    .collect()
            } else {
                samples
            }
        }
        Some("flac") => {
            let mut reader = claxon::FlacReader::open(path).unwrap();
            let info = reader.streaminfo();
            let sample_rate = info.sample_rate;
            let samples: Vec<f32> = reader
                .samples()
                .map(|s| s.unwrap() as f32 / i32::MAX as f32)
                .collect();

            // Resample to 16kHz if needed
            if sample_rate != SAMPLE_RATE {
                let ratio = sample_rate as f32 / SAMPLE_RATE as f32;
                samples
                    .iter()
                    .enumerate()
                    .filter_map(|(i, &s)| {
                        if (i as f32 % ratio) < 1.0 {
                            Some(s)
                        } else {
                            None
                        }
                    })
                    .collect()
            } else {
                samples
            }
        }
        _ => panic!("Unsupported audio format: {:?}", path),
    }
}
// // Audio processing
// fn load_wav(path: &Path) -> Vec<f32> {
//     let reader = WavReader::open(path).unwrap();
//     let spec = reader.spec();
//     let samples: Vec<f32> = reader
//         .into_samples::<i16>()
//         .map(|s| s.unwrap() as f32 / i16::MAX as f32)
//         .collect();

//     // Resample to 16kHz if needed (simplified - just takes every nth sample)
//     if spec.sample_rate != SAMPLE_RATE {
//         let ratio = spec.sample_rate as f32 / SAMPLE_RATE as f32;
//         samples
//             .iter()
//             .enumerate()
//             .filter_map(|(i, &s)| {
//                 if (i as f32 % ratio) < 1.0 {
//                     Some(s)
//                 } else {
//                     None
//                 }
//             })
//             .collect()
//     } else {
//         samples
//     }
// }

fn compute_mel_spectrogram(audio: &[f32]) -> Vec<Vec<f32>> {
    let mut planner = FftPlanner::new();
    let fft = planner.plan_fft_forward(N_FFT);
    let mut mel_frames = Vec::new();

    for i in (0..audio.len().saturating_sub(FRAME_SIZE)).step_by(HOP_SIZE) {
        let frame = &audio[i..i + FRAME_SIZE];

        let mut complex_buffer: Vec<Complex<f32>> = frame
            .iter()
            .enumerate()
            .map(|(j, &sample)| {
                let window = 0.5
                    - 0.5 * (2.0 * std::f32::consts::PI * j as f32 / (FRAME_SIZE - 1) as f32).cos();
                Complex::new(sample * window, 0.0)
            })
            .collect();

        complex_buffer.resize(N_FFT, Complex::new(0.0, 0.0));
        fft.process(&mut complex_buffer);

        let power_spectrum: Vec<f32> = complex_buffer[..N_FFT / 2]
            .iter()
            .map(|c| c.norm_sqr())
            .collect();

        let mut mel_frame = vec![0.0; N_MELS];
        for (i, &power) in power_spectrum.iter().enumerate() {
            let mel_idx = (i * N_MELS / power_spectrum.len()).min(N_MELS - 1);
            mel_frame[mel_idx] += power;
        }

        for val in mel_frame.iter_mut() {
            *val = (*val + 1e-10).log10();
        }

        mel_frames.push(mel_frame);
    }

    mel_frames
}

fn text_to_tokens(text: &str) -> Vec<i64> {
    text.chars()
        .filter_map(|c| {
            let c_lower = c.to_lowercase().to_string();
            VOCAB
                .iter()
                .position(|&v| v == c_lower)
                .map(|idx| idx as i64)
        })
        .collect()
}

// fn decode_predictions(predictions: Tensor<InferenceBackend, 3>) -> String {
//     let pred_data = predictions.argmax(2).into_data();
//     let indices = pred_data.as_slice::<i64>().unwrap();

//     let mut result = String::new();
//     for &idx in indices {
//         if let Some(&ch) = VOCAB.get(idx as usize) {
//             result.push_str(ch);
//         }
//     }
//     result
// }

// Training function
fn train_model(dataset_path: &str) {
    println!("🎯 Starting training...");

    let device = Default::default();
    let config = WhisperConfig {
        vocab_size: VOCAB_SIZE,
        feature_dim: N_MELS,
        hidden_dim: HIDDEN_DIM,
    };

    let mut model = TinyWhisper::<TrainBackend>::new(&config, &device);
    let mut optimizer = AdamConfig::new().init();

    // Load dataset
    println!("📁 Loading dataset from: {}", dataset_path);
    let mut training_data = Vec::new();

    // Expected structure: dataset_path/audio.wav and dataset_path/transcript.txt
    for (i, entry) in fs::read_dir(dataset_path).unwrap().enumerate() {
        if i > 800 {
            break;
        }
        let entry = entry.unwrap();
        let path = entry.path();

        if let Some(ext) = path.extension().and_then(|s| s.to_str()) {
            if ext == "wav" || ext == "flac" {
                // if path.extension().and_then(|s| s.to_str()) == Some("wav") {
                let transcript_path = path.with_extension("txt");
                if transcript_path.exists() {
                    let audio = load_audio(&path);
                    let transcript = fs::read_to_string(&transcript_path)
                        .unwrap()
                        .trim()
                        .to_lowercase();
                    let mel_frames = compute_mel_spectrogram(&audio);
                    let tokens = text_to_tokens(&transcript);

                    if !mel_frames.is_empty() && !tokens.is_empty() {
                        training_data.push((mel_frames, tokens));
                        println!("  ✓ Loaded: {} -> \"{}\"", path.display(), transcript);
                    }
                }
            }
        }
    }

    println!(
        "📊 Using {} training samples for faster training",
        training_data.len()
    );
    let lr = 0.003;
    // Training loop
    let epochs = 40;
    for epoch in 0..epochs {
        let mut total_loss = 0.0;
        let start_time = std::time::Instant::now();

        for (idx, (mel_frames, tokens)) in training_data.iter().enumerate() {
            // Show progress within epoch
            if idx % 100 == 0 && idx > 0 {
                print!(
                    "\rEpoch {}/{}: Processing sample {}/{}...",
                    epoch + 1,
                    epochs,
                    idx,
                    training_data.len()
                );
                use std::io::{self, Write};
                io::stdout().flush().unwrap();
            }

            let seq_len = mel_frames.len().min(tokens.len());
            if seq_len == 0 {
                continue;
            }

            // Prepare features
            let features_vec: Vec<f32> = mel_frames[..seq_len].iter().flatten().copied().collect();
            let features = Tensor::<TrainBackend, 3>::from_data(
                TensorData::new(features_vec, [1, seq_len, N_MELS]),
                &device,
            );

            // Prepare targets
            let targets = Tensor::<TrainBackend, 2, Int>::from_data(
                TensorData::new(tokens[..seq_len].to_vec(), [1, seq_len]),
                &device,
            );

            // Forward pass
            let logits = model.forward(features);
            let [b, s, v] = logits.dims();

            // Compute loss
            let loss = nn::loss::CrossEntropyLossConfig::new()
                .init(&device)
                .forward(logits.reshape([b * s, v]), targets.reshape([b * s]));

            // Backward pass
            let loss_val = loss.clone().mean().into_scalar();
            total_loss += loss_val;

            let grads = loss.backward();
            let grads = GradientsParams::from_grads(grads, &model);
            model = optimizer.step(lr, model, grads);
        }

        let epoch_time = start_time.elapsed();
        println!(
            "\nEpoch {}/{}: Loss = {:.4}, Time = {:.1}s",
            epoch + 1,
            epochs,
            total_loss / training_data.len() as f32,
            epoch_time.as_secs_f32()
        );

        // Estimate remaining time
        if epoch == 0 {
            let estimated_total = epoch_time.as_secs() * epochs as u64;
            println!("Estimated total time: {} minutes", estimated_total / 60);
        }
    }

    // Save model - Fixed with type annotation
    let recorder = DefaultFileRecorder::<FullPrecisionSettings>::new();
    model
        .save_file("model", &recorder)
        .expect("Failed to save model");
    println!("✅ Model saved to 'model.mpk'");
}

fn transcribe_realtime() {
    println!("🎤 Loading model and starting transcription...");

    // Load trained model
    let device = Default::default();
    let config = WhisperConfig {
        vocab_size: VOCAB_SIZE,
        feature_dim: N_MELS,
        hidden_dim: HIDDEN_DIM,
    };

    let recorder = DefaultFileRecorder::<FullPrecisionSettings>::new();
    let model = TinyWhisper::<InferenceBackend>::new(&config, &device)
        .load_file("model", &recorder, &device)
        .expect("Failed to load model. Train first with: cargo run -- train <dataset_path>");

    // Setup audio capture
    let audio_buffer = Arc::new(Mutex::new(VecDeque::<f32>::new()));
    let audio_buffer_clone = Arc::clone(&audio_buffer);

    // Track audio levels
    let audio_level = Arc::new(Mutex::new(0.0f32));
    let audio_level_clone = Arc::clone(&audio_level);

    let host = cpal::default_host();
    let input_device = host.default_input_device().expect("No input device");
    let device_name = input_device.name().unwrap_or("Unknown".to_string());

    println!("\n✅ Using device: {}", device_name);

    let input_config = input_device
        .default_input_config()
        .expect("Failed to get config");
    println!(
        "📈 Config: {} channels, {}Hz",
        input_config.channels(),
        input_config.sample_rate().0
    );

    println!("\n🎯 Listening... (Ctrl+C to stop)");
    println!("🔊 Audio Level: [Amplitude will show here]");
    println!("🐛 DEBUG MODE: Showing model internals\n");

    let channels = input_config.channels();

    let stream = match input_config.sample_format() {
        cpal::SampleFormat::F32 => {
            input_device
                .build_input_stream(
                    &input_config.into(),
                    move |data: &[f32], _: &_| {
                        // Convert to mono if needed
                        let mono_data: Vec<f32> = if channels > 1 {
                            data.chunks(channels as usize)
                                .map(|chunk| chunk.iter().sum::<f32>() / channels as f32)
                                .collect()
                        } else {
                            data.to_vec()
                        };

                        // Calculate RMS amplitude
                        let rms = (mono_data.iter().map(|&x| x * x).sum::<f32>()
                            / mono_data.len() as f32)
                            .sqrt();

                        *audio_level_clone.lock().unwrap() = rms;

                        let mut buffer = audio_buffer_clone.lock().unwrap();
                        buffer.extend(mono_data.iter());
                        while buffer.len() > SAMPLE_RATE as usize * 10 {
                            buffer.pop_front();
                        }
                    },
                    |err| eprintln!("Stream error: {}", err),
                    None,
                )
                .expect("Failed to build input stream")
        }
        _ => panic!("Unsupported format"),
    };

    stream.play().expect("Failed to start stream");

    // Transcription loop with debugging
    let mut last_processed = 0;
    let mut processing_count = 0;

    loop {
        std::thread::sleep(std::time::Duration::from_millis(500));

        let level = *audio_level.lock().unwrap();
        let db = 20.0 * (level + 1e-10).log10();

        println!("\n🔊 Audio Level: {:.3} ({:.1}dB)", level, db);

        let mut buffer = audio_buffer.lock().unwrap();
        let current_len = buffer.len();

        if current_len > last_processed + SAMPLE_RATE as usize / 2 {
            processing_count += 1;
            println!("📊 Processing chunk #{}", processing_count);

            let chunk: Vec<f32> = buffer.iter().copied().collect();
            drop(buffer);

            // Debug audio chunk
            let chunk_slice = &chunk[last_processed..];
            println!("  Audio chunk size: {} samples", chunk_slice.len());
            let chunk_rms =
                (chunk_slice.iter().map(|&x| x * x).sum::<f32>() / chunk_slice.len() as f32).sqrt();
            println!("  Chunk RMS: {:.3}", chunk_rms);

            let mel_frames = compute_mel_spectrogram(chunk_slice);
            println!(
                "  Mel frames: {} frames x {} features",
                mel_frames.len(),
                N_MELS
            );

            if !mel_frames.is_empty() {
                // Debug mel spectrogram
                let first_frame_energy: f32 = mel_frames[0].iter().sum();
                let last_frame_energy: f32 = mel_frames.last().unwrap().iter().sum();
                println!(
                    "  First frame energy: {:.2}, Last frame energy: {:.2}",
                    first_frame_energy, last_frame_energy
                );

                let features_vec: Vec<f32> = mel_frames.iter().flatten().copied().collect();
                let seq_len = mel_frames.len();

                println!("  Creating tensor: [1, {}, {}]", seq_len, N_MELS);
                let features = Tensor::<InferenceBackend, 3>::from_data(
                    TensorData::new(features_vec, [1, seq_len, N_MELS]),
                    &device,
                );

                println!("  Running model forward pass...");
                let predictions = model.forward(features);

                // Debug predictions
                let pred_shape = predictions.dims();
                println!("  Predictions shape: {:?}", pred_shape);

                // Get raw predictions for first few timesteps
                let pred_data = predictions.clone().into_data();
                let raw_preds = pred_data.as_slice::<f32>().unwrap();

                println!("  First timestep logits (top 5):");
                let first_timestep = &raw_preds[0..VOCAB_SIZE];
                let mut indexed: Vec<(usize, f32)> = first_timestep
                    .iter()
                    .enumerate()
                    .map(|(i, &v)| (i, v))
                    .collect();
                indexed.sort_by(|a, b| b.1.partial_cmp(&a.1).unwrap());

                for (idx, (vocab_idx, logit)) in indexed.iter().take(5).enumerate() {
                    let char = VOCAB.get(*vocab_idx).unwrap_or(&"?");
                    println!(
                        "    {}. '{}' (idx={}) = {:.2}",
                        idx + 1,
                        char,
                        vocab_idx,
                        logit
                    );
                }

                // Decode predictions
                println!("  Decoding predictions...");
                let decoded = decode_predictions(predictions.clone());
                println!("  Raw decoded text: '{}'", decoded);
                println!("  Decoded length: {} characters", decoded.len());

                // Show character-by-character predictions
                let argmax_data = predictions.argmax(2).into_data();
                let indices = argmax_data.as_slice::<i64>().unwrap();

                println!("  Character predictions (first 10):");
                for (i, &idx) in indices.iter().take(10).enumerate() {
                    let ch = VOCAB.get(idx as usize).unwrap_or(&"?");
                    println!("    Position {}: idx={} -> '{}'", i, idx, ch);
                }

                if !decoded.trim().is_empty() {
                    println!("\n✅ TRANSCRIPTION: '{}'", decoded);
                } else {
                    println!("\n⚠️  No non-empty transcription produced");
                }
            } else {
                println!("  ⚠️  No mel frames generated!");
            }

            last_processed = current_len - SAMPLE_RATE as usize / 4;
            println!(
                "  Next processing at: {} samples",
                last_processed + SAMPLE_RATE as usize / 2
            );
        }
    }
}

// Also update decode_predictions to handle potential issues
fn decode_predictions(predictions: Tensor<InferenceBackend, 3>) -> String {
    let pred_data = predictions.argmax(2).into_data();
    let indices = pred_data.as_slice::<i64>().unwrap();

    let mut result = String::new();
    let mut prev_idx = -1i64;

    for &idx in indices {
        // Simple deduplication - don't repeat same character
        if idx != prev_idx && idx >= 0 && (idx as usize) < VOCAB_SIZE {
            if let Some(&ch) = VOCAB.get(idx as usize) {
                result.push_str(ch);
            }
        }
        prev_idx = idx;
    }
    result
}
fn test_model_inference() {
    println!("🧪 Testing model inference...");

    let device = Default::default();
    let config = WhisperConfig {
        vocab_size: VOCAB_SIZE,
        feature_dim: N_MELS,
        hidden_dim: HIDDEN_DIM,
    };

    let recorder = DefaultFileRecorder::<FullPrecisionSettings>::new();
    let model = TinyWhisper::<InferenceBackend>::new(&config, &device)
        .load_file("model", &recorder, &device)
        .expect("Failed to load model");

    // Create dummy input
    println!("Creating test input: 10 frames of silence");
    let test_frames = vec![vec![-10.0; N_MELS]; 10]; // 10 frames of low energy
    let features_vec: Vec<f32> = test_frames.iter().flatten().copied().collect();

    let features = Tensor::<InferenceBackend, 3>::from_data(
        TensorData::new(features_vec, [1, 10, N_MELS]),
        &device,
    );

    println!("Running inference...");
    let predictions = model.forward(features);
    println!("Output shape: {:?}", predictions.dims());

    let decoded = decode_predictions(predictions.clone());
    println!("Decoded: '{}'", decoded);

    // Check what the model predicts for each position
    let argmax_data = predictions.argmax(2).into_data();
    let indices = argmax_data.as_slice::<i64>().unwrap();

    println!("\nPer-position predictions:");
    for (i, &idx) in indices.iter().enumerate() {
        let ch = VOCAB.get(idx as usize).unwrap_or(&"?");
        println!("  Position {}: {} -> '{}'", i, idx, ch);
    }
}

fn main() {
    let args: Vec<String> = std::env::args().collect();

    match args.get(1).map(|s| s.as_str()) {
        Some("train") => {
            if let Some(dataset_path) = args.get(2) {
                train_model(dataset_path);
            } else {
                eprintln!("Usage: cargo run -- train <dataset_path>");
            }
        }
        Some("transcribe") => transcribe_realtime(),
        // Some("test-mic") => test_microphone(),
        Some("test-model") => test_model_inference(), // New test
        _ => {
            println!("Whisper-like Speech Recognition");
            println!("Usage:");
            println!("  cargo run -- train <dataset_path>  # Train on dataset");
            println!("  cargo run -- transcribe            # Real-time transcription");
            // println!("  cargo run -- test-mic              # Test microphone");
            println!("  cargo run -- test-model            # Test model inference");
        }
    }
}
